{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import researchpy\n",
    "import matplotlib\n",
    "import ptitprince as pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(df, col1, col2, templates = None, DESC_VERBOSE=False, filter_n=None, GROUPBY_PERSONS = True, RETURN = False):\n",
    "    ''' Assumption col1 > col2 for the test metric. Ensure you pass col1 as imagined/biography vs col2 as recalled/autobiography. \n",
    "    Evaluates the sequentiality of the two columns and computes t-tests between them.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the columns to compare.\n",
    "        col1 (str): Name of the first group (e.g., imagined/biography). \n",
    "        col2 (str): Name of the second group (e.g., recalled/autobiography).\n",
    "        templates (list): List of string templates to automate computing difference for different parts of sequentiality (seq, topic, context). The df must have columns corresponding to all the templates after col1 and col2 are applied to the templates. \n",
    "        For example, if col1 is 'story_imagined' and col2 is 'story_recalled', the templates could be ['{}_seq', '{}_topic_seq', '{}_context_seq'] which would require the DataFrame to have columns like 'story_imagined_seq', 'story_recalled_seq', etc. necessarily.\n",
    "        \n",
    "        DESC_VERBOSE (bool): If True, prints descriptive statistics of the columns.\n",
    "        filter_n (int): Minimum number of entries per personality to keep in the DataFrame. Only applies if the DataFrame has a 'person' column.\n",
    "        GROUPBY_PERSONS (bool): If True, averages results across personalities. Only applicable if the dataframe has a 'person' column.\n",
    "        RETURN (bool): If True, returns the grouped DataFrame.\n",
    "    Returns:\n",
    "        None if RETURN is False, otherwise returns the grouped DataFrame.\n",
    "    Example:\n",
    "    >>> compute_metrics(df, 'story_imagined', 'story_recalled', templates=['{}_seq', '{}_topic_seq', '{}_context_seq'], DESC_VERBOSE=True, filter_n=20, GROUPBY_PERSONS=True)\n",
    "    '''\n",
    "\n",
    "    # Removing personalities with less than filter_n entries\n",
    "    if filter_n:\n",
    "        df = df.groupby('person').filter(lambda x: len(x) > filter_n)\n",
    "\n",
    "    if templates is None:\n",
    "        templates = ['{}_seq', '{}_topic_seq', '{}_context_seq']\n",
    "    print(templates)\n",
    "    # Descriptive statistics for all the templates across groups\n",
    "    if DESC_VERBOSE:\n",
    "        for template in templates:\n",
    "            print('-------------------------------------')\n",
    "            print(f'{template.format(col1)} description :\\n', df[template.format(col1)].describe()[['count', 'mean', 'std']])\n",
    "            print(f'{template.format(col2)} description :\\n', df[template.format(col2)].describe()[['count', 'mean', 'std']])\n",
    "            print('-------------------------------------')\n",
    "\n",
    "    # Difference computation for all the templates across groups\n",
    "    for template in templates:\n",
    "        df[template.format('seq_diff')] = df[template.format(col1)] - df[template.format(col2)]\n",
    "        print(f\"{template.format(col1)} and {template.format(col2)} ttest : {stats.ttest_rel(df[template.format(col1)], df[template.format(col2)])}\")\n",
    "        _, res = researchpy.ttest(df[template.format(col1)], df[template.format(col2)], paired=True)\n",
    "        display(res) \n",
    "    print('-------------------------------------')\n",
    "\n",
    "    if GROUPBY_PERSONS == False:\n",
    "        return\n",
    "    # Average seq_diff across each personality\n",
    "    for template in templates:\n",
    "        df[template.format('seq_diff')] = df[template.format(col1)] - df[template.format(col2)]\n",
    "        # Average across personality for both groups\n",
    "        df_grouped = df.groupby('person')[[template.format('seq_diff'),template.format(col1), template.format(col2)]].mean()\n",
    "        df_grouped = df_grouped.reset_index()\n",
    "\n",
    "        print(f\"Personality averaged {template.format('seq_diff')} t-test : {stats.ttest_1samp(df_grouped[template.format('seq_diff')], 0)}\")\n",
    "        _, res = researchpy.ttest(df_grouped[template.format(col1)], df_grouped[template.format(col2)], paired=True)\n",
    "        display(res)\n",
    "        print('-------------------------------------')\n",
    "\n",
    "        if RETURN:\n",
    "            return df_grouped\n",
    "\n",
    "\n",
    "def raincloud_plot(df, col1, col2, col1_name, col2_name, title, ignore_y_ticks = False):\n",
    "    pd.DataFrame.iteritems = pd.DataFrame.items # Without this somehow the ploting fails for raincloud plots\n",
    "    c1 = matplotlib.colors.hex2color('#0cc0df')\n",
    "    c2 = matplotlib.colors.hex2color('#ff3131')\n",
    "    # display(df[[col1, col2]])\n",
    "    pt.RainCloud(data=df[[col1, col2]], bw=0.05, cut=0, orient='v', palette=[c1, c2])\n",
    "    # plt.title(title)\n",
    "    plt.ylabel('Sequentiality', fontsize=16)\n",
    "    plt.xlabel('Type', fontsize=16)\n",
    "    if not ignore_y_ticks:\n",
    "        plt.yticks(np.arange(-0.5, 5.5, 1), fontsize=16)\n",
    "    plt.xticks([0, 1], [col1_name, col2_name], fontsize=16)\n",
    "    plt.savefig(f'../data/plots/{title}.svg', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function computes sequentiality difference across the constituent terms and displays the difference in terms of a paired t-test, with various descriptive stats. The requirement for using this function is to have a dataframe with columns corresponding to the two groups and the measures (seq, context and/or topic) you would like to compare across the two groups. Here is an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dir = '../data/scores/runV3/hcV3-stories_combined_scores_Meta-Llama-3.1-8B-Instruct-AWQ-INT4.csv'\n",
    "df = pd.read_csv(score_dir)\n",
    "print(df.columns)\n",
    "# >>> Index(['story_imagined', 'story_recalled', 'story_imagined_seq', 'story_imagined_topic_seq', 'story_imagined_context_seq', 'story_recalled_seq', 'story_recalled_topic_seq', 'story_recalled_context_seq'], dtype='object')\n",
    "\n",
    "# Compute metrics for the story imagined vs recalled\n",
    "df_grouped = compute_metrics(df, 'story_imagined', 'story_recalled', templates=['{}_seq', '{}_topic_seq', '{}_context_seq'], DESC_VERBOSE=True, RETURN=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also included code to plot the two side by side in a raincloud plot. Example (contd.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raincloud plot for the story imagined vs recalled\n",
    "raincloud_plot(df_grouped, 'story_imagined_seq', 'story_recalled_seq', 'Story Imagined', 'Story Recalled', 'Story Sequentiality')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
